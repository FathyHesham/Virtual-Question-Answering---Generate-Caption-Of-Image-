{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZWKUpalNp6b"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMz5SJUePeBI"
   },
   "outputs": [],
   "source": [
    "# Define GPU\n",
    "device_gpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2I-getQOp3p"
   },
   "outputs": [],
   "source": [
    "# Define Variable\n",
    "name_model = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\" : max_length, \"num_beams\" : num_beams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdqwydVYOgHs",
    "outputId": "00d246c2-0b15-47f5-8b3e-09f375588aab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.46.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Model & Feature Extraction & Tokenize\n",
    "model = VisionEncoderDecoderModel.from_pretrained(name_model)\n",
    "model = model.to(device_gpu)\n",
    "feature_extraction = ViTImageProcessor.from_pretrained(name_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIsJ0gc8QKZx"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps :\n",
    "  1. load the image\n",
    "  2. check the image is RGB or not\n",
    "  3. feature extraction for image\n",
    "  4. call the generate function to build the output \"caption\"\n",
    "  5. decode the output using tokenizer\n",
    "  6. return the output\n",
    "\"\"\"\n",
    "\n",
    "# create function to predict\n",
    "def predict_step(image_path):\n",
    "  # 1. load the image\n",
    "  image = Image.open(image_path)\n",
    "\n",
    "  # 2. check the image is RGB or not \" if RGB insert the image in list - else convert the image to RGB then inset the image in list\"\n",
    "  if image.mode != \"RGB\":\n",
    "    image = image.convert(mode = \"RGB\")\n",
    "\n",
    "  # 3. feature extraction for image\n",
    "  pixel_value = feature_extraction(images = image, return_tensors = \"pt\").pixel_values\n",
    "  pixel_value = pixel_value.to(device = device_gpu)\n",
    "\n",
    "  # 4. call the generate function to build the output \"caption\"\n",
    "  output = model.generate(pixel_value, **gen_kwargs)\n",
    "\n",
    "  # 5. decode the output using tokenizer\n",
    "  predicted_caption = tokenizer.batch_decode(output, skip_special_tokens = True)\n",
    "  predicted_caption = [pred.strip() for pred in predicted_caption]\n",
    "\n",
    "  # 6. return the output\n",
    "  return predicted_caption[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tlg9BETHknaG",
    "outputId": "6a8722d9-560c-404b-d156-99dee6558b49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children sitting around a table with a cake\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/content/image_1.jpg\"\n",
    "predicted_caption = predict_step(image_path)\n",
    "print(predicted_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QCslRy73T095",
    "outputId": "cf0f4674-4667-4b4b-d2ba-87c5effb0b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a woman standing in front of a grocery store filled with fresh produce\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/content/image_2.jpg\"\n",
    "predicted_caption = predict_step(image_path)\n",
    "print(predicted_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nk1Jrep8T2Yd",
    "outputId": "8e9ddb77-99e8-4ff8-b112-41bac8116d5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "men playing a game of soccer\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/content/image_3.jpg\"\n",
    "predicted_caption = predict_step(image_path)\n",
    "print(predicted_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBFizAaST3t-",
    "outputId": "d0a558f4-5ab8-42ec-9e5f-47f5741c2f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people are playing frisbee in a field\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/content/image_4.jpg\"\n",
    "predicted_caption = predict_step(image_path)\n",
    "print(predicted_caption)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "VQAENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
